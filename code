# This code  is related to the article
# Meyer and Wintenberger, "Sparse regular variation"

rm(list=ls())
library(rlang)
set.seed(123)

# 1. The sparse regular variation code

list_of_thresholds <- function(v){
  # This function gives the list of all thresholds for which
  # the vector v has a different number of nonnull coordinates
  d <- length(v)
  u <- sort(v, TRUE)
  threshold <- rep(0,d)
  su <- cumsum(u)
  
  for(k in 1:(d-1)){
    threshold[k] <- su[k]-k*u[k+1]}
  threshold[d] <- su[d] # threshold above which the projection does not make sense
  
  return(threshold)
}

cones_function <- function(v, Gamma){
  # This function gives the cones which contains the mass of Z
  # with Gamma by decreasing order
  d <- length(v)
  ord <- order(v)
  thres <- list_of_thresholds(v)
  length_Gamma <- length(Gamma)
  
  J <- which(Gamma < thres[d], arr.ind = TRUE) # the coordinates of the useful Gamma
  length_J <- length(J)
  # the one that are too big provide a vector of NA
  
  cones <- matrix(NA, nrow = d, ncol = length_Gamma)
  if (length_J > 0){
    # Initialization
    j <- J[1]
    r1 <- sum(thres<Gamma[j]) + 1 # number of positive coordinates for the threshold Gamma[1]
    cones[ord[1:(d-r1)], j] <- 0 # the coordinates on which v is not projected
    cones[ord[seq2(d-r1+1, d)], j] <- 1 # the coordinates on which v is projected
    
    j <- J[2]
    r2 <- d+2
  }
  
  if (length_J > 1){
    # the loop
    while ((r2 >= 1)&&(j<=length_Gamma)) {
      r2 <- sum(thres<Gamma[j]) + 1 # number of positive coordinates for the threshold Gamma[1]
      if (r2 <= d){
        cones[ , j] <- cones[ , j-1]
        cones[ord[seq2(d-r1+1, d-r2)], j] <- 0 # seq2 only works for increasing sequence which helps if r1 < r2
      } # we let the NA if the Gamma[1] is too huge
      j <- j + 1
      r1 <- r2
    }
  }
  return(cones)
  # the result cones gives for each threshold of Gamma a cone in which the projected vector belongs
}

algo_sparse <- function(X, Proportion, p){
  # This algo takes a matrix of data X, a list of proportion we want to keep Proportion, and a threshold parameter p,
  # and gives the cones on which the projection puts mass
  n <- ncol(X)
  length_Prop <- length(Proportion) # Proportion should be given in an increasing order
  d <- nrow(X)  
  
  sum_norm <- apply(X, 2, sum)
  sort_sum_norm <- sort(sum_norm, decreasing = TRUE)
  
  Thresholds <- sort_sum_norm[round(n*Proportion+1, 0)] # this corresponds to the list of thresholds
  # we use the function 'round' since otherwise there are some approximations in n*Proportion
  length_Thresholds <- length(Thresholds)
  
  X <- X[ , sum_norm>Thresholds[length_Prop]] # this is done only to avoid computing too many NA's
  
  # Initializations: the "adjacency matrix"
  binary <- apply(X, 2, cones_function, Thresholds)
  result <- list() # the return list
  
  for ( j in 1:length_Thresholds ){
    
    binary_bis <- binary[( (j-1)*d + 1 ): (j*d) , !is.na(binary[(j-1)*d+1, ]) ]
    # we remove the columns of NA of the considered block
    k <- ncol(binary_bis) # it corresponds to k_n
    M <- matrix(NA, nrow = d+1)
    
    # THE LOOP
    while (ncol(binary_bis) > 0){
      vect <- binary_bis[ , 1] # we take the first column vector of binary: we call it 'vect'
      vect_identical <- apply(binary_bis, 2, identical, vect) # we check in which position(s) 'vect' appears in 'binary_bis'
      numb_vect_identical <- sum(vect_identical) # we compute how many times 'vect' appears in 'binary_bis'
      M <- cbind(M, rbind (as.matrix(binary_bis[ , 1]), numb_vect_identical)) # we add this vector in a matrix M and add the number of occurence
      binary_bis <- as.matrix(binary_bis[ , -which(vect_identical==TRUE)]) # we remove 'vect' from 'binary_bis'
    }
    M <- M[ , -1] # because the first column is composed of NA (this should be done in a more elegant way)
    ordered_subcones <- order(M[d+1, ], decreasing=TRUE) # we order the matrix M in increasing order
    M <- as.matrix(M[ , ordered_subcones])
    
    # we delete the cones which are underrepresented
    non_signif_test <- M[d+1, ] > p*sum(M[d+1, ])/ncol(M)
    result[[j]] <- M[ , non_signif_test]
  }
  return( result )
}

# 2. The Damex code
# Epsilon-thickened rectangles, cf Goix, Sabourin, Clemencon
algo_damex <- function(X, Proportion, p, epsilon){
  n <- ncol(X)
  length_Prop <- length(Proportion) # Proportion should be given in an increasing order
  d <- nrow(X)  
  
  max_norm <- apply(X, 2, max)
  sort_max_norm <- sort(max_norm, decreasing = TRUE)
  Thresholds <- sort_max_norm[round(n*Proportion+1, 0)]
  length_Thresholds <- length(Thresholds)
  
  result <- list() # the return list
  
  for (j in 1:length_Thresholds){
    k <- sum(max_norm > Thresholds[j])
    zero <- matrix(rep(0,k*d), nrow = d) # the Null matrix
    
    binary <- pmax(X[ , max_norm > Thresholds[j] ] - n/k*epsilon, zero) > 0 # the epsilon-thickened rectangles
    M <- matrix(NA, nrow = d+1)

    while (ncol(binary) > 0){
      vect <- binary[ , 1] # we take the first column vector of binary: we call it 'vect'
      vect_identical <- apply(binary, 2, identical, vect) # we check in which position(s) vect' appears in 'binary'
      numb_vect_identical <- sum(vect_identical) # we compute how many times 'vect' appears in 'binary'
      M <- cbind(M, rbind (as.matrix(binary[ , 1]), numb_vect_identical))
      # we add this vector in a matrix M1, and add the number of occurence
      binary <- as.matrix(binary[ , -which(vect_identical==TRUE)]) # we remove 'vect' from 'binary'
    }
    M <- as.matrix(M[ , -1]) # because the first column is composed of NA (this should be done in a more elegant way)
    ordered_subcones <- order(M[d+1, ], decreasing=TRUE) # we order the matrix M in increasing order
    M <- as.matrix(M[ , ordered_subcones])
  
    # we delete the cones which are under-represented
    non_signif_test <- M[d+1, ] > p*sum(M[d+1, ])/ncol(M)
    result[[j]] <- as.matrix(M[, non_signif_test])
  }
  return( result )
}


#####
# NUMERICAL EXAMPLES
#####

N <- 100 # number of simulations
n <- c(10^4, 5*10^4, 10^5)
length_n <- length(n)
prop <- sqrt(n)/n # the proportion of extremes (k/n = sqrt(n)/n)
epsilon <- c(0.01, 0.1, 0.5, 1, 5, 10)
length_eps <- length(epsilon)


# 1. ASYMPTOTIC INDEPENDENCE
d1 <- 40 #dimension
M_theor <- diag(d1)

error1 <- matrix(rep(0, length_n*(length_eps+1)), nrow = length_n) # the error vector
time_compute1 <- matrix(rep(0, length_n*(length_eps+1)), nrow = length_n) # the time vector
colnames(error1) <- c("sparse", paste("e=",epsilon))
rownames(error1) <- paste("n=", n)
colnames(time_compute1) <- c("sparse", paste("e=",epsilon))
rownames(time_compute1) <- paste("n=", n)

for (r in 1:length_n){
  for (l in 1:N){
    X1 <- matrix(1/runif(n[r]*d1), nrow = d1) # the data: asymptotic independent coordinates
    
    ptm <- proc.time()
    sparse1 <- algo_sparse(X1, prop[r], p = 0.3)
    time_compute1[r, 1] <- (proc.time()-ptm)[3] + time_compute1[r, 1] # the time
    M_result <- sparse1[[1]][ -(d1+1), ]
    error1[r, 1] <- error1[r,1] + sum( tail( !duplicated( rbind(t(M_result), t(M_theor)) ), ncol(M_theor) ) ) + # those who are missing
      sum( tail( !duplicated( rbind(t(M_theor), t(M_result)) ), ncol(M_result) ) ) # those who should not be here
    
    damex1 <- list()
    for (j in 1:length_eps){
      ptm <- proc.time()
      damex1[[j]] <- algo_damex(X1, prop[r], p = 0.3, epsilon[j])
      time_compute1[r, j+1] <- (proc.time()-ptm)[3] + time_compute1[r, j+1] # the time
      M_result <- as.matrix(damex1[[j]][[1]][ -(d1+1), ])
      error1[r,j+1] <- error1[r,j+1] + sum( tail( !duplicated( rbind(t(M_result), t(M_theor)) ), ncol(M_theor) ) ) + # those who are missing
        sum( tail( !duplicated( rbind(t(M_theor), t(M_result)) ), ncol(M_result) ) ) # those who should not be here
    }
  }
}
error1/N
time_compute1



# 2. ASYMPTOTIC DEPENDENCE
d2 <- 20 #dimension

error2 <- matrix(rep(0, length_n*(length_eps+1)), nrow = length_n) # the error vector
time_compute2 <- matrix(rep(0, length_n*(length_eps+1)), nrow = length_n) # the time vector
colnames(error2) <- c("sparse", paste("e=", epsilon))
rownames(error2) <- paste("n=", n)
colnames(time_compute2) <- c("sparse", paste("e=",epsilon))
rownames(time_compute2) <- paste("n=", n)


M_theor <- matrix(0, nrow=d2, ncol=2)
M_theor[1:10,1] <- rep(1,10)
M_theor[11:20,2] <- rep(1,10)


for (r in 1:length_n){
  for (l in 1:N){
    # the construction of the data
    c_1 <- 1/runif(n[r]) # pareto(1)
    c_2 <- 100*c_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    c_3 <- 100*c_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    c_4 <- 100*c_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    c_5 <- 100*c_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    c_6 <- 0.01*c_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    c_7 <- 0.01*c_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    c_8 <- 0.01*c_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    c_9 <- 0.01*c_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    c_10 <- 0.01*c_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    X_dep1 <- rbind(c_1, c_2, c_3, c_4, c_5, c_6, c_7, c_8, c_9, c_10)
    
    e_1 <- 1/runif(n[r]) # pareto(1)
    e_2 <- 100*e_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    e_3 <- 100*e_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    e_4 <- 100*e_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    e_5 <- 100*e_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    e_6 <- 0.01*e_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    e_7 <- 0.01*e_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    e_8 <- 0.01*e_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    e_9 <- 0.01*e_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    e_10 <- 0.01*e_1 + 1/runif(n[r])^(1/2) # the same pareto but perturbed by a |Pareto(2)|
    X_dep2 <- rbind(e_1, e_2, e_3, e_4, e_5, e_6, e_7, e_8, e_9, e_10)
    
    X2 <- rbind(X_dep1, X_dep2)
    
    ptm <- proc.time()
    sparse2 <- algo_sparse(X2, prop[r], p = 0.1)
    time_compute2[r, 1] <- (proc.time()-ptm)[3] + time_compute2[r, 1] # the time
    M_result <- sparse2[[1]][ -(d2+1), ]
    error2[r,1] <- error2[r,1] + sum( tail( !duplicated( rbind(t(M_result), t(M_theor)) ), ncol(M_theor) ) ) + # those who are missing
      sum( tail( !duplicated( rbind(t(M_theor), t(M_result)) ), ncol(M_result) ) ) # those who should not be here
    
    damex2 <- list()
    for (j in 1:length_eps){
      ptm <- proc.time()
      damex2[[j]] <- algo_damex(X2, prop[r], p = 0.1, epsilon[j])
      time_compute2[r, j+1] <- (proc.time()-ptm)[3] + time_compute2[r, j+1] # the time
      M_result <- as.matrix(damex2[[j]][[1]][ -(d2+1), ])
      error2[r,j+1] <- error2[r,j+1] + sum( tail( !duplicated( rbind(t(M_result), t(M_theor)) ), ncol(M_theor) ) ) + # those who are missing
        sum( tail( !duplicated( rbind(t(M_theor), t(M_result)) ), ncol(M_result) ) ) # those who should not be here
    }
  }
}
error2/N
time_compute2
